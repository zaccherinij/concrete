use crate::aes::encryptors::AesCtr;
use std::cmp::Ordering;

/// Represents the counter used to index on the batch-generated bytes.
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub(crate) struct ByteCtr(pub u8);

/// Whether the next `increment` call will need a new batch of values or not.
pub(crate) enum ShouldGenerateBatch {
    GenerateBatch,
    Wait,
}

/// A state that uniquely defines the next byte outputted by a given `AesCtrGenerator`.
///
/// To construct a generator that can yield a byte at each call to `generate_next`, we must be
/// able to store both the last AES counter used, and the index of the last outputted byte in the
/// batch. This structure contains both, and provides operations to manipulate those two values
/// correctly.
#[derive(Debug, Copy, Clone)]
pub(crate) struct State {
    pub(crate) aes_ctr: AesCtr,
    pub(crate) byte_ctr: ByteCtr,
}

impl State {
    /// Generate a new state from an aes counter and a byte counter.
    #[allow(dead_code)]
    pub fn new(aes_ctr: AesCtr, byte_ctr: ByteCtr) -> Self {
        debug_assert!(byte_ctr.0 <= 127);
        State { aes_ctr, byte_ctr }
    }

    // Normalizes the current state, to be compatible with the offset of the other one.
    //
    // Two states with the same normalization are faster to compare.
    pub fn normalize_with(&mut self, other: &Self) {
        let State {
            aes_ctr: AesCtr(ref mut self_aes_ctr),
            byte_ctr: ByteCtr(ref mut self_byte_ctr),
        } = self;
        let State {
            aes_ctr: AesCtr(ref other_aes_ctr),
            ..
        } = other;
        let rem = (*self_aes_ctr - *other_aes_ctr) % 16;
        *self_aes_ctr -= rem;
        *self_byte_ctr += rem as u8 * 16;
    }

    // Return a clone of the state in normalized form, e.g. with the aes counter maximized, and
    // the byte counter minimized.
    pub fn to_normalized(self) -> Self {
        let State {
            aes_ctr: AesCtr(aes_ctr),
            byte_ctr: ByteCtr(byte_ctr),
        } = self;
        let aes_ctr = aes_ctr + (byte_ctr / 16) as u128;
        let byte_ctr = byte_ctr % 16;
        State {
            aes_ctr: AesCtr(aes_ctr),
            byte_ctr: ByteCtr(byte_ctr),
        }
    }

    /// Returns the successor of the current state.
    #[allow(dead_code)]
    pub(crate) fn successor(&self) -> Self {
        let State {
            byte_ctr: ByteCtr(byte_ctr),
            aes_ctr: AesCtr(aes_ctr),
        } = self;
        if *byte_ctr < 127 {
            State {
                aes_ctr: AesCtr(*aes_ctr),
                byte_ctr: ByteCtr(byte_ctr + 1),
            }
        } else {
            State {
                aes_ctr: AesCtr(*aes_ctr + 8),
                byte_ctr: ByteCtr(0),
            }
        }
    }

    /// Returns the byte counter.
    #[allow(dead_code)]
    pub(crate) fn get_byte_counter(&self) -> ByteCtr {
        self.byte_ctr
    }

    /// Returns the aes counter.
    pub fn get_aes_counter(&self) -> AesCtr {
        self.aes_ctr
    }

    /// Returns the current index on the batch.
    pub fn get_batch_index(&self) -> usize {
        self.byte_ctr.0 as usize
    }

    /// Increment the state.
    pub fn increment(&mut self) -> ShouldGenerateBatch {
        let State {
            aes_ctr: AesCtr(ref mut aes_ctr),
            byte_ctr: ByteCtr(ref mut byte_ctr),
        } = self;
        if *byte_ctr < 127 {
            *byte_ctr += 1;
            ShouldGenerateBatch::Wait
        } else {
            *aes_ctr += 8;
            *byte_ctr = 0;
            ShouldGenerateBatch::GenerateBatch
        }
    }

    /// Increments the state of several bytes at a time.
    pub fn shift(&mut self, n_bytes: usize) -> ShouldGenerateBatch {
        let State {
            aes_ctr: AesCtr(ref mut aes_ctr),
            byte_ctr: ByteCtr(ref mut byte_ctr),
        } = self;
        let n_bytes = n_bytes + (*byte_ctr as usize);
        let aes_bump = ((n_bytes / 128) as u128) * 8;
        *byte_ctr = (n_bytes % 128) as u8;
        if aes_bump > 0 {
            *aes_ctr += aes_bump;
            ShouldGenerateBatch::GenerateBatch
        } else {
            ShouldGenerateBatch::Wait
        }
    }
}

impl PartialEq for State {
    fn eq(&self, other: &Self) -> bool {
        let self_norm = self.to_normalized();
        let other_norm = other.to_normalized();
        self_norm.aes_ctr == other_norm.aes_ctr && self_norm.byte_ctr == other_norm.byte_ctr
    }
}

impl PartialOrd for State {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        let self_norm = self.to_normalized();
        let other_norm = other.to_normalized();
        if self_norm.aes_ctr < other_norm.aes_ctr {
            Some(Ordering::Less)
        } else if self_norm.aes_ctr > other_norm.aes_ctr {
            Some(Ordering::Greater)
        } else if self_norm.byte_ctr < other_norm.byte_ctr {
            Some(Ordering::Less)
        } else if self_norm.byte_ctr > other_norm.byte_ctr {
            Some(Ordering::Greater)
        } else {
            Some(Ordering::Equal)
        }
    }
}

impl Ord for State {
    fn cmp(&self, other: &Self) -> Ordering {
        self.partial_cmp(other).unwrap()
    }
}

impl Eq for State {}

impl Default for State {
    fn default() -> Self {
        State {
            aes_ctr: AesCtr(0),
            byte_ctr: ByteCtr(0),
        }
    }
}

impl From<AesCtr> for State {
    fn from(aes_ctr: AesCtr) -> Self {
        Self {
            aes_ctr,
            byte_ctr: ByteCtr(0),
        }
    }
}
